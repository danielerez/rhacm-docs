[#scale-in-worker-nodes]
= Scaling in a {sno} cluster with the {sco}

Scale in your managed cluster that was installed by the {sco}. You can scale in your cluster by removing a worker node.

*Required access:* Cluster administrator

[#scale-in-preq]
== Prerequisites

* If you are using GitOps ZTP, you have configured your GitOps ZTP environment. To configure your environment, see link:https://docs.redhat.com/en/documentation/openshift_container_platform/4.17/html/edge_computing/ztp-preparing-the-hub-cluster[Preparing the hub cluster for GitOps ZTP].
* You have the default templates. To get familiar with the default templates, see xref:../siteconfig/siteconfig_installation_templates.adoc#default-templates[Default set of templates]
* You have installed your cluster with the {sco}. To install a cluster with the {sco}, see xref:../siteconfig/siteconfig_install_clusters.adoc#install-clusters[Installing {sno} clusters with the {sco}]

[#scale-add-annotation]
== Adding an annotation to your worker node

Add an annotation to your worker node for removal.

Complete the following steps to annotate worker node from the managed cluster:

. Add an annotation in the `extraAnnotations` field of the worker node entry in the `ClusterInstance` custom resource that is used to provision your cluster:

+
[source,yaml]
----
spec:
   ...
   nodes:
   - hostName: "worker-node2.example.com"
      role: "worker"
      ironicInspect: ""
      extraAnnotations:
        BareMetalHost:
          bmac.agent-install.openshift.io/remove-agent-and-node-on-delete: "true"
...
----

. Apply the changes. See the following options:

.. If you are using {acm-short} without {gitops}, run the following command on the hub cluster:

+
[source,terminal]
----
oc apply -f <clusterinstance>.yaml
----

.. If you are using GitOps ZTP, push to your Git repository and wait for Argo CD to synchronize the changes.

. Verify that the annotation is applied to the `BaremetalHost` worker resource by running the following command on the hub cluster:

+
[source,terminal]
----
oc get bmh -n <clusterinstance_namespace> worker-node2.example.com -ojsonpath='{.metadata.annotations}' | jq
----

+
See the following example output for successful application of the annotation:
[source,terminal]
----
{
  "baremetalhost.metal3.io/detached": "assisted-service-controller",
  "bmac.agent-install.openshift.io/hostname": "worker-node2.example.com",
  "bmac.agent-install.openshift.io/remove-agent-and-node-on-delete": "true"
  "bmac.agent-install.openshift.io/role": "master",
  "inspect.metal3.io": "disabled",
  "siteconfig.open-cluster-management.io/sync-wave": "1",
}
----

[#scale-in-delete-baremetal-host]
== Deleting the `BareMetalHost` resource of the worker node

Delete the `BareMetalHost` resource of the worker node that you want to be removed.

Complete the following steps to remove a worker node from the managed cluster:

. Update the node object that you want to delete in your existing `ClusterInstance` custom resource with the following configuration:

+
[source,yaml]
----
...
spec:
   ...
   nodes:
     - hostName: "worker-node2.example.com"
       ...
       pruneManifests:
         - apiVersion: metal3.io/v1alpha1
           kind: BareMetalHost
...
----

. Apply the changes. See the following options.

.. If you are using {acm-short} without {gitops}, run the following command on the hub cluster:

+
[source,terminal]
----
oc apply -f <clusterinstance>.yaml
----

.. If you are using GitOps ZTP, push to your Git repository and wait for Argo CD to synchronize the changes.

. Verify that the `BareMetalHost` resources are removed by running the following command on the hub cluster:

+
[source,terminal]
----
oc get bmh -n <clusterinstance_namespace> --watch --kubeconfig <hub_cluster_kubeconfig_filename>
----

+
See the following example output:

+
[source,terminal]
----
NAME                        STATE                        CONSUMER         ONLINE   ERROR   AGE
master-node1.example.com    provisioned                  true             81m
worker-node2.example.com    deprovisioning               true             44m
worker-node2.example.com    powering off before delete   true             20h
worker-node2.example.com    deleting                     true             50m
----

. Verify that the `Agent` resources are removed by running the following command on the hub cluster:

+
[source,terminal]
----
oc get agents -n <clusterinstance_namespace> --kubeconfig <hub_cluster_kubeconfig_filename>
----

+
See the following example output:

+
[source,terminal]
----
NAME                       CLUSTER                  APPROVED   ROLE     STAGE
master-node1.example.com   <managed_cluster_name>   true       master   Done
master-node2.example.com   <managed_cluster_name>   true       master   Done
master-node3.example.com   <managed_cluster_name>   true       master   Done
worker-node1.example.com   <managed_cluster_name>   true       worker   Done
----

. Verify that the `Node` resources are removed by running the following command on the managed cluster:

+
[source,terminal]
----
oc get nodes --kubeconfig <managed_cluster_kubeconfig_filename>
----

+
See the following example output:

+
[source,terminal]
----
NAME                       STATUS                        ROLES                  AGE   VERSION
worker-node2.example.com   NotReady,SchedulingDisabled   worker                 19h   v1.30.5
worker-node1.example.com   Ready                         worker                 19h   v1.30.5
master-node1.example.com   Ready                         control-plane,master   19h   v1.30.5
master-node2.example.com   Ready                         control-plane,master   19h   v1.30.5
master-node3.example.com   Ready                         control-plane,master   19h   v1.30.5
----