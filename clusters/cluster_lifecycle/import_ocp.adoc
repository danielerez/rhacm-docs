[#import-ocp-cluster]
= Importing an on-premises {ocp} cluster manually by using {cim}

After you install {mce}, you are ready to import a managed cluster. You can import an existing {ocp-short} cluster so that you can add additional nodes. Continue reading the following topics to learn more:

* <<import-ocp-cluster-prereqs,Prerequisites>>
* <<import-ocp-cluster-steps,Importing a cluster>>
* <<import-ocp-cluster-resources,Importing cluster resources>>

[#import-ocp-cluster-prereqs]
== Prerequisites

- Enable the {cim} feature.

[#import-ocp-cluster-steps]
== Importing a cluster

Complete the following steps to import an {ocp-short} cluster manually, without a static network or a bare metal host, and prepare it for adding nodes:

. Create a namespace for the {ocp-short} cluster that you want to import by applying the following YAML content:
+
[source,yaml]
----
apiVersion: v1
kind: Namespace
metadata:
  name: managed-cluster
----

. Make sure that a ClusterImageSet matching the {ocp-short} cluster you are importing exists by applying the following YAML content:
+
[source,yaml]
----
apiVersion: hive.openshift.io/v1
kind: ClusterImageSet
metadata:
  name: openshift-v4.15
spec:
  releaseImage: quay.io/openshift-release-dev/ocp-release@sha256:22e149142517dfccb47be828f012659b1ccf71d26620e6f62468c264a7ce7863
----

. Add your pull secret to access the image by applying the following YAML content:
+
[source,yaml]
----
apiVersion: v1
kind: Secret
type: kubernetes.io/dockerconfigjson
metadata:
  name: pull-secret
  namespace: managed-cluster
stringData:
  .dockerconfigjson: <pull-secret-json> <1>
----
+
<1> Replace <pull-secret-json> with your pull secret JSON.

. Copy the `kubeconfig` from your {ocp-short} cluster to the hub cluster.

.. Get the `kubeconfig` from your {ocp-short} cluster by running the following command. Make sure that `kubeconfig` is set as the cluster being imported:
+
----
oc get secret -n openshift-kube-apiserver node-kubeconfigs -ojson | jq '.data["lb-ext.kubeconfig"]' --raw-output | base64 -d > /tmp/kubeconfig.some-other-cluster
----
+
*Note:* If your cluster API is accessed through a custom domain, you must first edit this `kubeconfig` by adding your custom certificates in the `certificate-authority-data` field and by changing the `server` field to match your custom domain.
+
.. Copy the `kubeconfig` to the hub cluster by running the following command. Make sure that `kubeconfig` is set as your hub cluster:
+
----
oc -n managed-cluster create secret generic some-other-cluster-admin-kubeconfig --from-file=kubeconfig=/tmp/kubeconfig.some-other-cluster
----

. Create an `AgentClusterInstall` custom resource by applying the following YAML content. Replace values where needed:
+
[source,yaml]
----
apiVersion: extensions.hive.openshift.io/v1beta1
kind: AgentClusterInstall
metadata:
  name: <your-cluster-name> <1>
  namespace: <managed-cluster>
spec:
  networking:
    userManagedNetworking: true
  clusterDeploymentRef:
    name: <your-cluster>
  imageSetRef:
    name: openshift-v4.11.18
  provisionRequirements:
    controlPlaneAgents: <2>
  sshPublicKey: <""> <3> 
----
+
<1> Choose a name for your cluster.
<2> Use `1` if you are using a {sno} cluster. Use `3` if you are using a multinode cluster.
<3> Add the optional `sshPublicKey` field to log in to nodes for troubleshooting.

. Create a `ClusterDeployment` by applying the following YAML content. Replace values where needed:
+
[source,yaml]
----
apiVersion: hive.openshift.io/v1
kind: ClusterDeployment
metadata:
  name: <your-cluster-name> <1>
  namespace: managed-cluster
spec:
  baseDomain: <redhat.com> <2>
  installed: <true> <3>
  clusterMetadata:
      adminKubeconfigSecretRef:
        name: <your-cluster-name-admin-kubeconfig> <4>
      clusterID: <""> <5>
      infraID: <""> <5>
  clusterInstallRef:
    group: extensions.hive.openshift.io
    kind: AgentClusterInstall
    name: your-cluster-name-install
    version: v1beta1
  clusterName: your-cluster-name
  platform:
    agentBareMetal:
  pullSecretRef:
    name: pull-secret
----
+
<1> Choose a name for your cluster.
<2> Make sure `baseDomain` matches the domain you are using for your {ocp-short} cluster.
<3> Set to `true` to automatically import your {ocp-short} cluster  as a production environment cluster.
<4> Reference the `kubeconfig` you created in step 4.
<5> Leave `clusterID` and `infraID` empty in production environments.

. Add an `InfraEnv` custom resource to discover new hosts to add to your cluster by applying the following YAML content. Replace values where needed:
+
*Note:* The following example might require additional configuration if you are not using a static IP address.
+
[source,yaml]
----
apiVersion: agent-install.openshift.io/v1beta1
kind: InfraEnv
metadata:
  name: your-infraenv
  namespace: managed-cluster
spec:
  clusterRef:
    name: your-cluster-name
    namespace: managed-cluster
  pullSecretRef:
    name: pull-secret
  sshAuthorizedKey: ""
----

.InfraEnv field table
|===
| Field | Optional or required | Description

| `clusterRef`
| Optional
| The `clusterRef` field is optional if you are using late binding. If you are not using late binding, you must add the `clusterRef`.

| `sshAuthorizedKey`
| Optional
| Add the optional `sshAuthorizedKey` field to log in to nodes for troubleshooting.
|===

. If the import is successful, a URL to download an ISO file appears. Download the ISO file by running the following command, replacing <url> with the URL that appears:
+
*Note:* You can automate host discovery by using bare metal host.
+
----
oc get infraenv -n managed-cluster some-other-infraenv -ojson | jq ".status.<url>" --raw-output | xargs curl -k -o /storage0/isos/some-other.iso
----

. *Optional:* If you want to use {acm-short} features, such as policies, on your {ocp-short} cluster, create a `ManagedCluster` resource. Make sure that the name of your `ManagedCluster` resource matches the name of your `ClusterDeplpoyment` resource. If you are missing the `ManagedCluster` resource, your cluster status is `detached` in the console.

[#import-ocp-cluster-resources]
== Importing cluster resources

If your {ocp-short} managed cluster was installed by the {ai}, you can move the managed cluster and its resources from one hub cluster to another hub cluster. 

You can manage a cluster from a new hub cluster by saving a copy of the original resources, applying them to the new hub cluster, and deleting the original resources. You can then scale down or scale up your managed cluster from the new hub cluster.

*Important:* You can only scale down imported {ocp-short} managed clusters if they were installed by the {ai}.

[#import-ocp-cluster-resources-table]
=== Manage cluster resource table

You can import the following resources and continue to manage your cluster with them:

|===
| Resource | Optional or required | Description

| `Agent`
| Required
| 

| `AgentClassification`
| Optional
| 

| `AgentClusterInstall`
| Required
| 

| `BareMetalHost`
| Required
| 

| `ClusterDeployment`
| Required
| 

| `InfraEnv`
| Required
| 

| `NMStateConfig`
| Optional
| 

| `ManagedCluster`
| Required
| 

| `Secret`
| Required
| Includes `admin-kubeconfig` and `bmc-secret`
|===

[#save-apply-cluster-resources]
=== Saving and applying managed cluster resources

To save a copy of your managed cluster resources and apply them to a new hub cluster, complete the following steps:

. Get your resources from your source hub cluster by running the following command. Repeat the command for every resource you want to import by replacing `<resource_name>` with the name of the resource. Replace other values where needed:

+
[source,bash]
----
oc –kubeconfig <source_hub_kubeconfig> -n <managed_cluster_name> get <resource_name> <cluster_provisioning_namespace> -oyaml > <resource_name>.yaml
----

. Remove the `ownerReferences` property from the following resources by running the following commands:
+
.. `AgentClusterInstall`
+
[source,bash]
----
yq --in-place -y 'del(.metadata.ownerReferences)' AgentClusterInstall.yaml
----
+
.. `Secret` (`admin-kubeconfig`)
+
[source,bash]
----
yq --in-place -y 'del(.metadata.ownerReferences)' AdminKubeconfigSecret.yaml
----

. Detach the managed cluster from the source hub cluster by running the following command to your `ManagedCluster` resource. Replace values where needed:

+
[source,bash]
----
oc –kubeconfig <target_hub_kubeconfig> delete ManagedCluster <cluster_name>
----

. Create a namespace on the target hub cluster for the managed cluster. Use a similar name as the source hub cluster.

. Apply your stored resources on the target hub cluster individually by running the following command. Repalce values where needed:
+
*Note:* Replace `<resource_name>.yaml` with `.` if you want to apply all the resources as a group instead of individually.
+
[source,bash]
----
oc –kubeconfig <target_hub_kubeconfig> apply -f <resource_name>.yaml
----

[#remove-cluster-source-hub]
=== Removing the managed cluster from the source hub cluster

After importing your cluster resources, remove your managed cluster from the source hub cluster by completing the following steps:

. Set the `spec.preserveOnDelete` parameter to `true` in the `ClusterDeployment` custom resource to prevent destroying the managed cluster.

. Complete the steps in xref:../cluster_lifecycle/remove_managed_cluster.adoc#remove-managed-cluster[Removing a cluster from management].
